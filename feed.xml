<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://konradkalita.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://konradkalita.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-03T16:40:12+00:00</updated><id>https://konradkalita.github.io/feed.xml</id><title type="html">Konrad Kalita</title><subtitle>My personal thought on ML and AI. </subtitle><entry><title type="html">December 2023 in ML</title><link href="https://konradkalita.github.io/blog/2024/december_2023/" rel="alternate" type="text/html" title="December 2023 in ML"/><published>2024-01-03T00:00:00+00:00</published><updated>2024-01-03T00:00:00+00:00</updated><id>https://konradkalita.github.io/blog/2024/december_2023</id><content type="html" xml:base="https://konradkalita.github.io/blog/2024/december_2023/"><![CDATA[<h2 id="intro">Intro</h2> <p>Despite the fact that the dust from OpenAI board <a href="https://www.wired.com/story/sam-altman-openai-back/">drama</a> had fallen, December was another hot month for a Deep Learning community. Two major conferences took place, dozens of interesting papers were published and every week a new LLM was announced. Below is my short summary of things which caught my eye in the last month.</p> <h2 id="conferences">Conferences</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/2023_12_december/poster-480.webp 480w, /assets/img/2023_12_december/poster-800.webp 800w, /assets/img/2023_12_december/poster-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/2023_12_december/poster.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/2023_12_december/poster_hall-480.webp 480w, /assets/img/2023_12_december/poster_hall-800.webp 800w, /assets/img/2023_12_december/poster_hall-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/2023_12_december/poster_hall.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/2023_12_december/disney_bot_wide-480.webp 480w, /assets/img/2023_12_december/disney_bot_wide-800.webp 800w, /assets/img/2023_12_december/disney_bot_wide-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/2023_12_december/disney_bot_wide.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Neurips provided a lot of interesting posters and some cool gadgets at companies' booths. </div> <p>December was a month of two large conferences: <a href="https://2023.emnlp.org">EMNLP</a> and <a href="https://nips.cc/">Neurips</a>. I had the pleasure to participate in the latter one and have some reflections about ML conferences in general. As time went by, the popularity of ML went through the roof. On the other hand, COVID time forced researchers to move collaboration to social networks or private channels and the status of conferences had to change. But, as usual with big organizations, conferences have problems to adjust on time. If you happen to work in a trending area like NLP or Computer Vision then most of the interesting papers are already discussed on X / Reddit / Discord and you probably read them about 6-9 months ago. Nevertheless, events like Neurips are a good occasion to synthesize knowledge and have a bird‚Äôs-eye view on a whole field. Here are my takeaways from this year:</p> <ul> <li>In terms of model architectures: Latent Diffusion Models and State Space Language Models are all the rage right now</li> <li>Alignment via RLHF methods seems the main point of focus for NLP working on LLMs labs/startups</li> <li>Theory of Deep Learning slowly but steadily moves forward. It was nice to see that phenomena assigned exclusively to large models / overparameterization like Double Descent can be reproduced in Linear Models. That said, there is still no mathematical framework for reasoning about DL models generalization without unrealistic behavior like data orthogonality, etc.</li> <li>There were much more that I would expect papers in applications for biology (proteins folding, drug discovery, etc.)</li> <li>The bar for technical sophistication of accepted papers was lower than usual this year. Interesting experiments with evaluation via OpenAI API were enough.</li> <li>All technical aspects of LLMs like: inference time, context window, memory requirements or fine-tuning FLOPs budget improved tremendously this year</li> <li>LoRA started separate line of research on its own</li> <li>Even researchers doing very theoretical work like Johnatan Frankle (author of Lottery Ticket Hypothesis) are working now in industry on LLMs</li> </ul> <h2 id="new-llms">New LLMs</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/2023_12_december/llm_meme-480.webp 480w, /assets/img/2023_12_december/llm_meme-800.webp 800w, /assets/img/2023_12_december/llm_meme-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/2023_12_december/llm_meme.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="gemini">Gemini</h3> <p>December was another crazy month for NLP. Google announced their new model family - <a href="https://blog.google/technology/ai/google-gemini-ai">Gemini</a> - which will supersede PALM-2 models. The benchmarks show more or less the parity of the smaller Gemini Pro model with GPT-3.5 and larger Gemini Ultra with GPT-4. Although the results suggest that Google will be able to mostly close the gap to OpenAI offerings, the presentation was overshadowed by a fake <a href="https://techcrunch.com/2023/12/07/googles-best-gemini-demo-was-faked">demo</a> scandal.</p> <p><strong>Why does it matter:</strong> Although Google was and still is a leader in ML/AI research they lagged behind OpenAI in terms of LLM via API offerings. They needed a whole year to catch up but it seems that in 2024 the choice of best LLM for the job will not be as simple as before.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/2023_12_december/gemini_bench-480.webp 480w, /assets/img/2023_12_december/gemini_bench-800.webp 800w, /assets/img/2023_12_december/gemini_bench-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/2023_12_december/gemini_bench.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="mixtral">Mixtral</h3> <p>When I was trying to get my head around over 3k posters on Neurips, Mistral.ai published their new model <a href="https://mistral.ai/news/mixtral-of-experts/">Mixtral</a>. Interestingly in contrast to <a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral-7B</a> the new model uses MoE<d-footnote>MoE (Mixture of Experts): a transformer model with separate subnetworks which are used to learn different task - check <a href="https://arxiv.org/abs/2101.03961">Switch Transformer</a> paper</d-footnote> approach. It promises performance equal to the largest Llama 2 70B model while using only 8B parameters at inference time. Unfortunately, as usual with MoE the price for inference speed are huge memory requirements. The model weights are open-sourced and can be downloaded from <a href="https://huggingface.co/docs/transformers/model_doc/mixtral">HuggingFace</a>. It also has integration with <a href="https://github.com/vllm-project/vllm">vLLM</a> runtime environment.</p> <p><strong>Why does it matter:</strong> Despite dozens of open-source LLMs being published this year there is still a significant gap in performance between open-source and proprietary models. Mixtral seems to be an important step in narrowing that gap.</p> <h3 id="phi">Phi</h3> <p>While Microsoft is relying on its partnership with OpenAI for providing best in class LLMs, S√©bastien Bubeck‚Äôs team in Microsoft Research is working on the line of relatively small language models. This month they released <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">Phi-2</a> which with only 2.9B parameters achieves very good scores on popular benchmarks (comparable to Mistral-7B). The main point of interest for Phi models is actually composition of instruction fine-tuning dataset which is further described in their paper <a href="https://arxiv.org/abs/2306.11644">Textbooks are all you need</a>.</p> <p><strong>Why does it matter:</strong> It is still an open problem how much the model size can be amortized with data of better quality and vice versa.</p> <h3 id="tinyllama">TinyLlama</h3> <p>Another notable mention in the realm of not so large language models is the upload of the final checkpoint of TinyLlama project <a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T">HuggingFace</a>.</p> <h2 id="papers-tools-and-misc">Papers, Tools and Misc</h2> <h3 id="2023-review">2023 Review</h3> <p>As 2023 had come to an end a bunch of ‚ÄúYear in Review‚Äù posts came out. I especially liked to of them:</p> <ul> <li><a href="https://arxiv.org/abs/2312.09323">Perspectives on the State and Future of Deep Learning - 2023</a></li> <li><a href="https://twitter.com/g_leech_/status/1740027508727464312">Gavin Leech 2023 Summary</a></li> </ul> <h3 id="mlx-and-sigma-œÉreparam">MLX and sigma œÉReparam</h3> <p>Apple released their own Machine Learning framework <a href="https://github.com/ml-explore/mlx">MLX</a> and interesting work on hyperparameter tuning for LLMs training <a href="https://github.com/apple/ml-sigma-reparam">œÉReparam</a>.</p> <p><strong>Why does it matter:</strong> Apple is a huge player in ML but until this year they keep most things closed. They do not publish papers and do not contribute to open-source ecosystems. Now it seems that there is some change in their behavior. It is a little bit ironic though as some time ago Chris Lattner moved from Apple to Google to build next-gen ML framework <a href="https://arxiv.org/abs/2102.13243">Swift for Tensorflow</a> which unfortunately was not widely adopted. Now Apple builds their own framework in C++ with Python bindings like everyone else. It seems there is no space for language other than Python in ML right now and the near future.</p> <h3 id="barrelrec">BarrelRec</h3> <p>Inspired by some Neurips discussion <a href="https://twitter.com/francoisfleuret">Fran√ßois Fleuret</a> started working on his own memory efficient transformer modification called BarrelRec which seems to get some traction, although I would recommend some dose of skepticism as in the past a lot of so called efficient transformer architectures showed some real problem with scaling comparing to vanilla transformer.</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">i&#39;ve always wanted to do this...<br/><br/>BarrelRec trains 10x FASTER than conventional QKV attention!! ü§Øü§ØüöÄ <a href="https://t.co/dEvayrvfj3">pic.twitter.com/dEvayrvfj3</a></p>&mdash; Dimitri von R√ºtte (@dvruette) <a href="https://twitter.com/dvruette/status/1740144820918923406?ref_src=twsrc%5Etfw">December 27, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <h3 id="state-space-language-models">State Space Language Models</h3> <p>If you somehow missed the fuss about State Space Language Models Nathaniel Lambert wrote a great <a href="https://www.interconnects.ai/p/llms-beyond-attention">post</a> about recent advancements.</p> <p><strong>Why does it matter:</strong> There are exciting applications for Language Models like Neural Turing Machine or Genetics where the ability to process long sequences in linear time and memory is a must-have. That said, the investment of money and time into optimizing transformer architectures is so huge that a new architecture must be really strong contentenders to see any adoption.</p> <h3 id="auto-4-bit-quantization">Auto 4-bit quantization</h3> <p>Casper Hansen released <a href="https://github.com/casper-hansen/AutoAWQ">AutoAWQ</a> which promises to effectively quantize models to 4-bits.</p> <h3 id="stable-video-diffusion">Stable Video Diffusion</h3> <p>2022 was the year of image generation, 2023 is the year of text generation, it seems that 2024 may be the year of video generative models or at least a lot of people wish it would be. To make it happen Stability just dropped a new <a href="https://stability.ai/news/introducing-stable-video-diffusion-api">model</a> to their dev platform.</p> <h3 id="prompting-guide">Prompting guide</h3> <p>As LLMs availability is growing, our awareness of good prompting techniques is much more solid. This report from Bsharat et al. seems to do nice job in presenting most of the tricks: <a href="https://huggingface.co/papers/2312.16171">paper</a></p> <h3 id="december-2023-papers">December 2023 Papers</h3> <details><summary>Papers I looked at this month (mostly on Neurips)</summary> <ul> <li>Batched Low-Rank Adaptation of Foundation Models<d-cite key="wenBatchedLowRankAdaptation2023"></d-cite></li> <li>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models<d-cite key="singhHumanDataScaling2023"></d-cite></li> <li>Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine<d-cite key="noriCanGeneralistFoundation2023"></d-cite></li> <li>Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs<d-cite key="liCanLLMAlready2023"></d-cite></li> <li>D4: Improving LLM Pretraining via Document De-Duplication and Diversification<d-cite key="tirumalaD4ImprovingLLM2023"></d-cite></li> <li>DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction<d-cite key="pourrezaDINSQLDecomposedInContext2023"></d-cite></li> <li>Distributed Inference and Fine-tuning of Large Language Models Over The Internet<d-cite key="borzunovDistributedInferenceFinetuning2023"></d-cite></li> <li>Don‚Äôt Stop Pretraining: Adapt Language Models to Domains and Tasks<d-cite key="gururanganDonStopPretraining2020a"></d-cite></li> <li>DORIS-MAE: Scientific Document Retrieval Using Multi-level Aspect-based Queries<d-cite key="wangDORISMAEScientificDocument2023"></d-cite></li> <li>Generate What You Prefer: Reshaping Sequential Recommendation via Guided Diffusion<d-cite key="yangGenerateWhatYou2023"></d-cite></li> <li>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources<d-cite key="wangHowFarCan2023"></d-cite></li> <li>HyperAttention: Long-context Attention in Near-Linear Time<d-cite key="hanHyperAttentionLongcontextAttention2023"></d-cite></li> <li>Improving CLIP Training with Language Rewrites<d-cite key="fanImprovingCLIPTraining2023"></d-cite></li> <li>Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts<d-cite key="tulchinskiiIntrinsicDimensionEstimation2023"></d-cite></li> <li>Joint Prompt Optimization of Stacked LLMs Using Variational Inference<d-cite key="sordoniJointPromptOptimization2023"></d-cite></li> <li>Larger Language Models Do In-Context Learning Differently<d-cite key="weiLargerLanguageModels2023"></d-cite></li> <li>Mamba: Linear-Time Sequence Modeling with Selective State Spaces<d-cite key="guMambaLinearTimeSequence2023"></d-cite></li> <li>Meta-in-Context Learning in Large Language Models<d-cite key="coda-fornoMetaincontextLearningLarge2023"></d-cite></li> <li>Mind2Web: Towards a Generalist Agent for the Web<d-cite key="dengMind2WebGeneralistAgent2023"></d-cite></li> <li>Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment<d-cite key="luthNavigatingPitfallsActive2023"></d-cite></li> <li>Preference-Grounded Token-level Guidance for Language Model Fine-tuning<d-cite key="yangPreferencegroundedTokenlevelGuidance2023"></d-cite></li> <li>Recommender Systems with Generative Retrieval<d-cite key="rajputRecommenderSystemsGenerative2023"></d-cite></li> <li>Rethinking the Role of Token Retrieval in Multi-Vector Retrieval<d-cite key="leeRethinkingRoleToken2023"></d-cite></li> <li>Scaling MLPs: A Tale of Inductive Bias<d-cite key="bachmannScalingMLPsTale2023"></d-cite></li> <li>ToolQA: A Dataset for LLM Question Answering with External Tools<d-cite key="zhuangToolQADatasetLLM2023"></d-cite></li> <li>Training Chain-of-Thought via Latent-Variable Inference<d-cite key="phanTrainingChainofThoughtLatentVariable2023"></d-cite></li> </ul> </details> ]]></content><author><name>Konrad Kalita</name></author><category term="ml"/><category term="review"/><category term="december"/><category term="2023"/><category term="llm"/><category term="neurips"/><summary type="html"><![CDATA[Neurips, LLMs and other stuff]]></summary></entry></feed>