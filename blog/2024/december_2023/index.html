<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>December 2023 in ML | Konrad Kalita</title> <meta name="author" content="Konrad Kalita"> <meta name="description" content="Neurips, LLMs and other stuff"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://konradkalita.github.io/blog/2024/december_2023/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "December 2023 in ML",
      "description": "Neurips, LLMs and other stuff",
      "published": "January 3, 2024",
      "authors": [
        {
          "author": "Konrad Kalita",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Personal",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Konrad Kalita</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>December 2023 in ML</h1> <p>Neurips, LLMs and other stuff</p> </d-title><d-byline></d-byline><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#intro">Intro</a></div> <div><a href="#conferences">Conferences</a></div> <div><a href="#new-llms">New LLMs</a></div> <ul> <li><a href="#gemini">Gemini</a></li> <li><a href="#mixtral">Mixtral</a></li> <li><a href="#phi">Phi</a></li> <li><a href="#tinyllama">TinyLlama</a></li> </ul> <div><a href="#papers-tools-and-misc">Papers, Tools and Misc</a></div> <ul> <li><a href="#2023-review">2023 Review</a></li> <li><a href="#barrelrec">BarrelRec</a></li> <li><a href="#state-space-language-models">State Space Language Models</a></li> <li><a href="#auto-4-bit-quantization">Auto 4-bit quantization</a></li> <li><a href="#stable-video-diffusion">Stable Video Diffusion</a></li> <li><a href="#prompting-guide">Prompting guide</a></li> <li><a href="#december-2023-papers">December 2023 Papers</a></li> </ul> </nav> </d-contents> <h2 id="intro">Intro</h2> <p>Despite the fact that the dust from OpenAI board <a href="https://www.wired.com/story/sam-altman-openai-back/" rel="external nofollow noopener" target="_blank">drama</a> had fallen, December was another hot month for a Deep Learning community. Two major conferences took place, dozens of interesting papers were published and every week a new LLM was announced. Below is my short summary of things which caught my eye in the last month.</p> <h2 id="conferences">Conferences</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/2023_12_december/poster-480.webp 480w, /assets/img/2023_12_december/poster-800.webp 800w, /assets/img/2023_12_december/poster-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023_12_december/poster.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/2023_12_december/poster_hall-480.webp 480w, /assets/img/2023_12_december/poster_hall-800.webp 800w, /assets/img/2023_12_december/poster_hall-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023_12_december/poster_hall.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/2023_12_december/disney_bot_wide-480.webp 480w, /assets/img/2023_12_december/disney_bot_wide-800.webp 800w, /assets/img/2023_12_december/disney_bot_wide-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023_12_december/disney_bot_wide.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Neurips provided a lot of interesting posters and some cool gadgets at companies' booths. </div> <p>December was a month of two large conferences: <a href="https://2023.emnlp.org" rel="external nofollow noopener" target="_blank">EMNLP</a> and <a href="https://nips.cc/" rel="external nofollow noopener" target="_blank">Neurips</a>. I had the pleasure to participate in the latter one and have some reflections about ML conferences in general. As time went by, the popularity of ML went through the roof. On the other hand, COVID time forced researchers to move collaboration to social networks or private channels and the status of conferences had to change. But as usual with big organizations they have problems to adjust on time. If you happen to work in a hot area like NLP or CV then most of the interesting papers are already discussed on X / Reddit / Discord and you probably read them about 6-9 months ago. Nevertheless, events like Neurips are a good occasion to synthesize knowledge and have a bird-eye perspective on a whole field. Here are my takeaways from this year:</p> <ul> <li>In terms of model architectures: Latent Diffusion Models and State Space Language Models are all the rage right now</li> <li>Alignment via RLHF methods seems the main point of focus for NLP working on LLMs labs/startups</li> <li>Theory of Deep Learning slowly but steadily moves forward. It was nice to see that phenomena assigned exclusively to large models / overparameterization like Double Descent can be reproduced in Linear Models. That said, there is still no mathematical framework for reasoning about DL models generalization without unrealistic behavior like data orthogonality, etc.</li> <li>There were much more that I would expect papers in applications for biology (proteins folding, drug discovery, etc.)</li> <li>The bar for technical sophistication of accepted papers was lower than usual this year. Interesting experiments with evaluation via OpenAI API were enough.</li> <li>All technical aspects of LLMs like: inference time, context window, memory requirements or fine-tuning FLOPs budget improved tremendously this year</li> <li>LoRA started separate line of research on its own</li> <li>Even researchers doing very theoretical work like Johnatan Frankle (author of Lottery Ticket Hypothesis) are working now in industry on LLMs</li> </ul> <h2 id="new-llms">New LLMs</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/2023_12_december/llm_meme-480.webp 480w, /assets/img/2023_12_december/llm_meme-800.webp 800w, /assets/img/2023_12_december/llm_meme-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023_12_december/llm_meme.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="gemini">Gemini</h3> <p>December was another crazy month for NLP. Google announced their new model family - <a href="https://blog.google/technology/ai/google-gemini-ai" rel="external nofollow noopener" target="_blank">Gemini</a> - which will supersede PALM-2 models. The benchmarks show more or less the parity of the smaller Gemini Pro model with GPT-3.5 and larger Gemini Ultra with GPT-4. Although the results suggest that Google will be able to mostly close the gap to OpenAI offerings, the presentation was overshadowed by a fake <a href="https://techcrunch.com/2023/12/07/googles-best-gemini-demo-was-faked" rel="external nofollow noopener" target="_blank">demo</a> scandal.</p> <p><strong>Why does it matter:</strong> Although Google was and still is a leader in ML/AI research they lagged behind OpenAI in terms of LLM via API offerings. They needed a whole year to catch up but it seems that in 2024 the choice of best LLM for the job will not be as simple as before.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/2023_12_december/gemini_bench-480.webp 480w, /assets/img/2023_12_december/gemini_bench-800.webp 800w, /assets/img/2023_12_december/gemini_bench-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023_12_december/gemini_bench.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="mixtral">Mixtral</h3> <p>When I was trying to get my head around over 3k posters on Neurips, Mistral.ai published their new model <a href="https://mistral.ai/news/mixtral-of-experts/" rel="external nofollow noopener" target="_blank">Mixtral</a>. Interestingly in contrast to <a href="https://mistral.ai/news/announcing-mistral-7b/" rel="external nofollow noopener" target="_blank">Mistral-7B</a> the new model uses MoE<d-footnote>MoE (Mixture of Experts): a transformer model with separate subnetworks which are used to learn different task - check <a href="https://arxiv.org/abs/2101.03961" rel="external nofollow noopener" target="_blank">Switch Transformer</a> paper</d-footnote> approach. It promises performance equal to the largest Llama 2 70B model while using only 8B parameters at inference time. Unfortunately, as usual with MoE the price for inference speed are huge memory requirements. The model weights are open-sourced and can be downloaded from <a href="https://huggingface.co/docs/transformers/model_doc/mixtral" rel="external nofollow noopener" target="_blank">HuggingFace</a>. It also has integration with <a href="https://github.com/vllm-project/vllm" rel="external nofollow noopener" target="_blank">vLLM</a> runtime environment.</p> <p><strong>Why does it matter:</strong> Despite dozens of open-source LLMs being published this year there is still a significant gap in performance between open-source and proprietary models. Mixtral seems to be an important step in narrowing that gap.</p> <h3 id="phi">Phi</h3> <p>While Microsoft is relying on its partnership with OpenAI for providing best in class LLMs, Sebastien Bubecks‚Äôs team in Microsoft Research is working on the line of relatively small language models. This month they released <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/" rel="external nofollow noopener" target="_blank">Phi-2</a> which with only 2.9B parameters achieves very good scores on popular benchmarks (comparable to Mistral-7B). The main point of interest for Phi models is actually composition of instruction fine-tuning dataset which is further described in their paper <a href="https://arxiv.org/abs/2306.11644" rel="external nofollow noopener" target="_blank">Textbooks are all you need</a>.</p> <p><strong>Why does it matter:</strong> It is still an open problem how much the model size can be amortized with data of better quality and vice versa.</p> <h3 id="tinyllama">TinyLlama</h3> <p>Another notable mention in the realm of not so large language models is the upload of the final checkpoint of TinyLlama project <a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T" rel="external nofollow noopener" target="_blank">HuggingFace</a>.</p> <h2 id="papers-tools-and-misc">Papers, Tools and Misc</h2> <h3 id="2023-review">2023 Review</h3> <p>As 2023 had come to an end a bunch of ‚ÄúYear in Review‚Äù posts came out. I especially liked to of them:</p> <ul> <li><a href="https://arxiv.org/abs/2312.09323" rel="external nofollow noopener" target="_blank">Perspectives on the State and Future of Deep Learning - 2023</a></li> <li><a href="https://twitter.com/g_leech_/status/1740027508727464312" rel="external nofollow noopener" target="_blank">Gavin Leech 2023 Summary</a></li> </ul> <h3 id="mlx-and-sigma-œÉreparam">MLX and sigma œÉReparam</h3> <p>Apple released their own Machine Learning framework <a href="https://github.com/ml-explore/mlx" rel="external nofollow noopener" target="_blank">MLX</a> and interesting work on hyperparameter tuning for LLMs training <a href="https://github.com/apple/ml-sigma-reparam" rel="external nofollow noopener" target="_blank">œÉReparam</a>.</p> <p><strong>Why does it matter:</strong> Apple is a huge player in ML but until this year they keep most things closed. They do not publish papers and do not contribute to open-source ecosystems. Now it seems that there is some change in their behavior. It is a little bit ironic though as some time ago Chris Lattner moved from Apple to Google to build next-gen ML framework <a href="https://arxiv.org/abs/2102.13243" rel="external nofollow noopener" target="_blank">Swift for Tensorflow</a> which unfortunately was not widely adopted. Now Apple builds their own framework in C++ with Python bindings like everyone else. It seems there is no space for language other than Python in ML right now and the near future.</p> <h3 id="barrelrec">BarrelRec</h3> <p>Inspired by some Neurips discussion <a href="https://twitter.com/francoisfleuret" rel="external nofollow noopener" target="_blank">Fran√ßois Fleuret</a> started working on his own memory efficient transformer modification called BarrelRec which seems to get some traction, although I would recommend some dose of skepticism as in the past a lot of so called efficient transformer architectures showed some real problem with scaling comparing to vanilla transformer.</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">i've always wanted to do this...<br><br>BarrelRec trains 10x FASTER than conventional QKV attention!! ü§Øü§ØüöÄ <a href="https://t.co/dEvayrvfj3" rel="external nofollow noopener" target="_blank">pic.twitter.com/dEvayrvfj3</a></p>‚Äî Dimitri von R√ºtte (@dvruette) <a href="https://twitter.com/dvruette/status/1740144820918923406?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">December 27, 2023</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <h3 id="state-space-language-models">State Space Language Models</h3> <p>If you somehow missed the fuss about State Space Language Models Nathaniel Lambert wrote a great <a href="https://www.interconnects.ai/p/llms-beyond-attention" rel="external nofollow noopener" target="_blank">post</a> about recent advancements.</p> <p><strong>Why does it matter:</strong> There are exciting applications for Language Models like Neural Turing Machine or Genetics where the ability to process long sequences in linear time and memory is a must-have. That said, the investment of money and time into optimizing transformer architectures is so huge that a new architecture must be really strong contentenders to see any adoption.</p> <h3 id="auto-4-bit-quantization">Auto 4-bit quantization</h3> <p>Casper Hansen released <a href="https://github.com/casper-hansen/AutoAWQ" rel="external nofollow noopener" target="_blank">AutoAWQ</a> which promises to effectively quantize models to 4-bits.</p> <h3 id="stable-video-diffusion">Stable Video Diffusion</h3> <p>2022 was the year of image generation, 2023 is the year of text generation, it seems that 2024 may be the year of video generative models or at least a lot of people wish it would be. To make it happen Stability just dropped a new <a href="https://stability.ai/news/introducing-stable-video-diffusion-api" rel="external nofollow noopener" target="_blank">model</a> to their dev platform.</p> <h3 id="prompting-guide">Prompting guide</h3> <p>As LLMs availability is growing, our awareness of good prompting techniques is much more solid. This report from Bsharat et al. seems to do nice job in presenting most of the tricks: <a href="https://huggingface.co/papers/2312.16171" rel="external nofollow noopener" target="_blank">paper</a></p> <h3 id="december-2023-papers">December 2023 Papers</h3> <details><summary>Papers I looked at this month (mostly on Neurips)</summary> <ul> <li>Batched Low-Rank Adaptation of Foundation Models<d-cite key="wenBatchedLowRankAdaptation2023"></d-cite> </li> <li>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models<d-cite key="singhHumanDataScaling2023"></d-cite> </li> <li>Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine<d-cite key="noriCanGeneralistFoundation2023"></d-cite> </li> <li>Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs<d-cite key="liCanLLMAlready2023"></d-cite> </li> <li>D4: Improving LLM Pretraining via Document De-Duplication and Diversification<d-cite key="tirumalaD4ImprovingLLM2023"></d-cite> </li> <li>DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction<d-cite key="pourrezaDINSQLDecomposedInContext2023"></d-cite> </li> <li>Distributed Inference and Fine-tuning of Large Language Models Over The Internet<d-cite key="borzunovDistributedInferenceFinetuning2023"></d-cite> </li> <li>Don‚Äôt Stop Pretraining: Adapt Language Models to Domains and Tasks<d-cite key="gururanganDonStopPretraining2020a"></d-cite> </li> <li>DORIS-MAE: Scientific Document Retrieval Using Multi-level Aspect-based Queries<d-cite key="wangDORISMAEScientificDocument2023"></d-cite> </li> <li>Generate What You Prefer: Reshaping Sequential Recommendation via Guided Diffusion<d-cite key="yangGenerateWhatYou2023"></d-cite> </li> <li>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources<d-cite key="wangHowFarCan2023"></d-cite> </li> <li>HyperAttention: Long-context Attention in Near-Linear Time<d-cite key="hanHyperAttentionLongcontextAttention2023"></d-cite> </li> <li>Improving CLIP Training with Language Rewrites<d-cite key="fanImprovingCLIPTraining2023"></d-cite> </li> <li>Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts<d-cite key="tulchinskiiIntrinsicDimensionEstimation2023"></d-cite> </li> <li>Joint Prompt Optimization of Stacked LLMs Using Variational Inference<d-cite key="sordoniJointPromptOptimization2023"></d-cite> </li> <li>Larger Language Models Do In-Context Learning Differently<d-cite key="weiLargerLanguageModels2023"></d-cite> </li> <li>Mamba: Linear-Time Sequence Modeling with Selective State Spaces<d-cite key="guMambaLinearTimeSequence2023"></d-cite> </li> <li>Meta-in-Context Learning in Large Language Models<d-cite key="coda-fornoMetaincontextLearningLarge2023"></d-cite> </li> <li>Mind2Web: Towards a Generalist Agent for the Web<d-cite key="dengMind2WebGeneralistAgent2023"></d-cite> </li> <li>Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment<d-cite key="luthNavigatingPitfallsActive2023"></d-cite> </li> <li>Preference-Grounded Token-level Guidance for Language Model Fine-tuning<d-cite key="yangPreferencegroundedTokenlevelGuidance2023"></d-cite> </li> <li>Recommender Systems with Generative Retrieval<d-cite key="rajputRecommenderSystemsGenerative2023"></d-cite> </li> <li>Rethinking the Role of Token Retrieval in Multi-Vector Retrieval<d-cite key="leeRethinkingRoleToken2023"></d-cite> </li> <li>Scaling MLPs: A Tale of Inductive Bias<d-cite key="bachmannScalingMLPsTale2023"></d-cite> </li> <li>ToolQA: A Dataset for LLM Question Answering with External Tools<d-cite key="zhuangToolQADatasetLLM2023"></d-cite> </li> <li>Training Chain-of-Thought via Latent-Variable Inference<d-cite key="phanTrainingChainofThoughtLatentVariable2023"></d-cite> </li> </ul> </details> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/dec2023.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2024 Konrad Kalita. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>